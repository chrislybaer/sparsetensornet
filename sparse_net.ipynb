{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2018 Chris Wendler\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow with Sparse Tensors\n",
    "\n",
    "This notebook implements a fully connected neural network layer parametrized by a sparse tensor. The layer takes a sparse tensor as input and outputs another sparse tensor. Note that for this toy implementation the nonzero indices of the input tensor must coincide with the ones of the weight tensor. The goal of this notebook is to show how to write custom operations with gradients for sparse tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: XOR - problem\n",
    "\n",
    "We create a dataset with sparse tensors (with fixed nonzero entries) as input and binary labels depending on the first two non-zero components of the sparse tensor are as output. Sample $x$ has label $x_0 \\mbox{ xor } x_1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 5) (5000,) 0.5044\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"/tmp/xor/model.ckpt\"\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 200\n",
    "N = 5000\n",
    "D = 1000\n",
    "index = np.ones((5, 1))\n",
    "index[:, 0] = np.random.randint(0,D,5)\n",
    "#indices = np.asarray([index]*N)\n",
    "values = np.ones((N, 5))\n",
    "values[:,:4] = np.random.rand(N,4)*2 - 1\n",
    "shape = [D]\n",
    "labels = ((values[:,0] < 0) != (values[:, 1] < 0)).astype(np.int32)\n",
    "print(values.shape, labels.shape, labels.sum()/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wrap the data into a tf.data.Dataset, in order to feed it to our model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data\n",
    "x_train = tf.data.Dataset.from_tensor_slices(values[:int(0.8*N)])\n",
    "y_train = tf.data.Dataset.from_tensor_slices(labels[:int(0.8*N)]).map(lambda y: tf.one_hot(y, 2))\n",
    "dataset = tf.data.Dataset.zip((x_train, y_train))      \n",
    "dataset = dataset.map(lambda x, y: (tf.SparseTensor(index, x, shape), y), 6)\n",
    "dataset = dataset.shuffle(1000).repeat(NUM_EPOCHS).batch(BATCH_SIZE)\n",
    "#test data\n",
    "x_test = tf.data.Dataset.from_tensor_slices(values[int(0.8*N):])\n",
    "y_test = tf.data.Dataset.from_tensor_slices(labels[int(0.8*N):]).map(lambda y: tf.one_hot(y, 2))\n",
    "testset = tf.data.Dataset.zip((x_test, y_test))      \n",
    "testset = testset.map(lambda x, y: (tf.SparseTensor(index, x, shape), y), 6)\n",
    "testset = testset.batch(BATCH_SIZE)\n",
    "#data iterator\n",
    "iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes, output_classes=(tf.SparseTensor, tf.Tensor))\n",
    "next_element = iterator.get_next()\n",
    "train_init_op = iterator.make_initializer(dataset)\n",
    "test_init_op = iterator.make_initializer(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Ops using Pyfunc\n",
    "\n",
    "For any practical application this part should be implemented using a C++ kernel, however, for this toy example it suffices to implement the custom operations necessary for the implementation of the sparse tensor fully connected layer using tf.py_func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "\n",
    "# Define custom py_func which takes also a grad op as argument:\n",
    "def py_func(func, inp, Tout, stateful=True, name=None, grad=None):\n",
    "    # Need to generate a unique name to avoid duplicates:\n",
    "    rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+8))\n",
    "    tf.RegisterGradient(rnd_name)(grad)  \n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({\"PyFunc\": rnd_name}):\n",
    "        return tf.py_func(func, inp, Tout, stateful=stateful, name=name)\n",
    "    \n",
    "def dense_np(w_idx, w_val, w_shape, x_idx, x_val, x_shape):\n",
    "    \"\"\"\n",
    "    Computes fully connected layer for sparse tensors. Nonzero indices must be in matching order.\n",
    "    Args:\n",
    "        w ... the sparse weight tensor [n_neurons, n_nonzeros_in]\n",
    "        x ... the sparse input batch tensor [batch_size, n_nonzeros_in]\n",
    "    \"\"\"\n",
    "    batch_size = x_shape[0]\n",
    "    n_neurons = w_shape[0]\n",
    "    h_shape = np.array([batch_size, n_neurons])\n",
    "    x = np.arange(batch_size)\n",
    "    y = np.arange(n_neurons) \n",
    "    h_idx = np.transpose([np.repeat(x, len(y)), np.tile(y, len(x))])\n",
    "    h_vals = np.zeros(batch_size * n_neurons)\n",
    "    \n",
    "    for bi in range(batch_size):\n",
    "        #for ni in range(n_neurons):\n",
    "        #    h_vals[bi*n_neurons + ni] = np.dot(w_val[w_idx[:,0] == ni], x_val[x_idx[:,0] == bi])\n",
    "        for idx, val in zip(w_idx, w_val):\n",
    "            h_vals[bi*n_neurons + idx[0]] += val * x_val[(x_idx == [bi, idx[1]]).all(axis=1)]      \n",
    "    return h_idx.astype(np.int64), h_vals.astype(np.float32), h_shape.astype(np.int64)\n",
    "\n",
    "def dense_grad_np(w_idx, w_val, w_shape, x_idx, x_val, x_shape, grad):\n",
    "    batch_size = x_shape[0]\n",
    "    n_neurons = w_shape[0]\n",
    "    w_grad = np.zeros(w_val.shape)\n",
    "    x_grad = np.zeros(x_val.shape)\n",
    "    \n",
    "    for j, idx in enumerate(w_idx):\n",
    "        for bi in range(batch_size):\n",
    "            w_grad[j] += grad[bi*n_neurons + idx[0]]*x_val[(x_idx==[bi, idx[1]]).all(axis=1)]\n",
    "            \n",
    "    for k, idx in enumerate(x_idx):\n",
    "        for ni in range(n_neurons):\n",
    "            x_grad[k] += grad[idx[0]*n_neurons + ni]*w_val[(w_idx==[ni, idx[1]]).all(axis=1)]\n",
    "            \n",
    "    return w_grad.astype(np.float32), x_grad.astype(np.float32)\n",
    "\n",
    "def dense_grad_pyfunc(w_idx, w_val, w_shape, x_idx, x_val, x_shape, grad, name=None):\n",
    "    with ops.name_scope(name, \"dense_grad_pyfunc\", [w_idx, w_val, w_shape, x_idx, x_val, x_shape, grad]) as name:\n",
    "        return tf.py_func(dense_grad_np, [w_idx, w_val, w_shape, x_idx, x_val, x_shape, grad], [tf.float32, tf.float32], name=name)\n",
    "\n",
    "def dense_grad(op, grad1, grad2, grad3):\n",
    "    w_idx = op.inputs[0]\n",
    "    w_val = op.inputs[1]\n",
    "    w_shape = op.inputs[2]\n",
    "    x_idx = op.inputs[3]\n",
    "    x_val = op.inputs[4]\n",
    "    x_shape = op.inputs[5]\n",
    "    w_grad, x_grad = dense_grad_pyfunc(w_idx, w_val, w_shape, x_idx, x_val, x_shape, grad2)\n",
    "    return None, w_grad, None, None, x_grad, None\n",
    "\n",
    "def dense_op(w_idx, w_val, w_shape, x_idx, x_val, x_shape, name=None):\n",
    "    with ops.name_scope(name, \"dense_pyfunc\", [w_idx, w_val, w_shape, x_idx, x_val, x_shape]) as name:\n",
    "        return py_func(dense_np, [w_idx, w_val, w_shape, x_idx, x_val, x_shape], [tf.int64, tf.float32, tf.int64],\n",
    "                       name = name, grad = dense_grad)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the Custom Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.layers import base\n",
    "from tensorflow.python.layers import utils\n",
    "class SparseFullyConnected(base.Layer):\n",
    "    def __init__(self, units, units_in, in_indices, activation, to_dense=False, name=None, trainable=True, **kwargs):\n",
    "        super(SparseFullyConnected, self).__init__(trainable=trainable, name=name, **kwargs)\n",
    "        self.n_units_out = units\n",
    "        self.n_units_in = units_in\n",
    "        self.activation = activation\n",
    "        self.in_indices = in_indices\n",
    "        self.to_dense = to_dense\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #expected input format: [None, dim]\n",
    "        n_nonzero = len(self.in_indices)\n",
    "        with tf.variable_scope(self.name, reuse=True) as scope:\n",
    "            self.w_values = self.add_variable('W', shape=[self.n_units_out * n_nonzero], dtype=tf.float32, \n",
    "                                     initializer=tf.glorot_uniform_initializer(), trainable=True)\n",
    "        self.w_shape = [self.n_units_out, self.n_units_in]\n",
    "        x = np.arange(self.n_units_out)\n",
    "        y = self.in_indices\n",
    "        self.w_indices = np.transpose([np.repeat(x, len(y)), np.tile(y, len(x))])\n",
    "        self.W = tf.SparseTensor(self.w_indices, self.w_values, self.w_shape)\n",
    "        self.built = True\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        w_idx = self.W.indices\n",
    "        w_val = self.W.values\n",
    "        w_shape = self.W.dense_shape\n",
    "        x_idx = inputs.indices\n",
    "        x_val = inputs.values\n",
    "        x_shape = inputs.dense_shape\n",
    "        h_idx, h_val, h_shape = dense_op(w_idx, w_val, w_shape, x_idx, x_val, x_shape)\n",
    "        #h = tf.SparseTensor(h_idx, h_val, h_shape)\n",
    "        h = tf.SparseTensor(h_idx, self.activation(h_val), h_shape)\n",
    "        if(self.to_dense):\n",
    "            h = tf.sparse_add(tf.zeros(h_shape), h) \n",
    "        return h\n",
    "        \n",
    "def sparsefc(inputs, units, units_in, in_indices, activation, to_dense=False, name=None, reuse=None):\n",
    "    layer = SparseFullyConnected(units, units_in, in_indices, activation, to_dense=to_dense, name=name, dtype=inputs.dtype.base_dtype, _scope=name, _reuse=reuse)\n",
    "    return layer.apply(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defintion of the Model and Training/Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(in_data):\n",
    "    in_indices = index.reshape((-1))\n",
    "    fc1 = sparsefc(in_data, 10, D, in_indices, tf.nn.relu)\n",
    "    in_indices = np.arange(10)\n",
    "    fc2 = sparsefc(fc1, 10, 10, in_indices, tf.nn.relu)\n",
    "    fc3 = sparsefc(fc2, 2, 10, in_indices, tf.nn.relu, to_dense=True)\n",
    "    return fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = nn_model(next_element[0])\n",
    "# add the optimizer and loss\n",
    "loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=next_element[1], logits=logits))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(train_init_op)\n",
    "    writer = tf.summary.FileWriter(\"/tmp/xor\", sess.graph)\n",
    "    sess.run(loss)\n",
    "    writer.close()\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "# get accuracy\n",
    "prediction = tf.argmax(logits, 1)\n",
    "equality = tf.equal(prediction, tf.argmax(next_element[1], 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'sparse_fully_connected/W:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'sparse_fully_connected_1/W:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'sparse_fully_connected_2/W:0' shape=(20,) dtype=float32_ref>\n",
      "<tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>\n",
      "<tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>\n",
      "<tf.Variable 'sparse_fully_connected/W/Adam:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'sparse_fully_connected/W/Adam_1:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'sparse_fully_connected_1/W/Adam:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'sparse_fully_connected_1/W/Adam_1:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'sparse_fully_connected_2/W/Adam:0' shape=(20,) dtype=float32_ref>\n",
      "<tf.Variable 'sparse_fully_connected_2/W/Adam_1:0' shape=(20,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "for var in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n",
    "    print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, loss: 138.608, training accuracy: 48.00%\n",
      "W change:  4.9998966e-05\n",
      "Model saved in path: /tmp/xor/model.ckpt-0\n",
      "Batch: 10, loss: 138.493, training accuracy: 48.00%\n",
      "W change:  0.0024471374\n",
      "Model saved in path: /tmp/xor/model.ckpt-10\n",
      "Batch: 20, loss: 138.073, training accuracy: 53.00%\n",
      "W change:  0.0024763562\n",
      "Model saved in path: /tmp/xor/model.ckpt-20\n",
      "Batch: 30, loss: 138.208, training accuracy: 53.50%\n",
      "W change:  0.0025087628\n",
      "Model saved in path: /tmp/xor/model.ckpt-30\n",
      "Batch: 40, loss: 138.094, training accuracy: 54.50%\n",
      "W change:  0.0027642709\n",
      "Model saved in path: /tmp/xor/model.ckpt-40\n",
      "Batch: 50, loss: 137.687, training accuracy: 56.50%\n",
      "W change:  0.0031841423\n",
      "Model saved in path: /tmp/xor/model.ckpt-50\n",
      "Batch: 60, loss: 136.970, training accuracy: 59.00%\n",
      "W change:  0.0039656097\n",
      "Model saved in path: /tmp/xor/model.ckpt-60\n",
      "Batch: 70, loss: 136.564, training accuracy: 64.50%\n",
      "W change:  0.005355005\n",
      "Model saved in path: /tmp/xor/model.ckpt-70\n",
      "Batch: 80, loss: 135.973, training accuracy: 67.50%\n",
      "W change:  0.0057190955\n",
      "Model saved in path: /tmp/xor/model.ckpt-80\n",
      "Batch: 90, loss: 134.544, training accuracy: 65.50%\n",
      "W change:  0.0058738524\n",
      "Model saved in path: /tmp/xor/model.ckpt-90\n",
      "Batch: 100, loss: 132.707, training accuracy: 71.50%\n",
      "W change:  0.005842865\n",
      "Model saved in path: /tmp/xor/model.ckpt-100\n",
      "Batch: 110, loss: 130.361, training accuracy: 65.50%\n",
      "W change:  0.006076169\n",
      "Model saved in path: /tmp/xor/model.ckpt-110\n",
      "Batch: 120, loss: 130.164, training accuracy: 68.00%\n",
      "W change:  0.0058559733\n",
      "Model saved in path: /tmp/xor/model.ckpt-120\n",
      "Batch: 130, loss: 127.677, training accuracy: 71.00%\n",
      "W change:  0.0061270855\n",
      "Model saved in path: /tmp/xor/model.ckpt-130\n",
      "Batch: 140, loss: 124.333, training accuracy: 70.50%\n",
      "W change:  0.006402181\n",
      "Model saved in path: /tmp/xor/model.ckpt-140\n",
      "Batch: 150, loss: 123.125, training accuracy: 71.50%\n",
      "W change:  0.006400647\n",
      "Model saved in path: /tmp/xor/model.ckpt-150\n",
      "Batch: 160, loss: 122.563, training accuracy: 68.00%\n",
      "W change:  0.0059059216\n",
      "Model saved in path: /tmp/xor/model.ckpt-160\n",
      "Batch: 170, loss: 119.363, training accuracy: 71.50%\n",
      "W change:  0.0059171245\n",
      "Model saved in path: /tmp/xor/model.ckpt-170\n",
      "Batch: 180, loss: 112.437, training accuracy: 79.50%\n",
      "W change:  0.005762231\n",
      "Model saved in path: /tmp/xor/model.ckpt-180\n",
      "Batch: 190, loss: 112.867, training accuracy: 74.00%\n",
      "W change:  0.005863383\n",
      "Model saved in path: /tmp/xor/model.ckpt-190\n",
      "Batch: 200, loss: 109.557, training accuracy: 76.50%\n",
      "W change:  0.005583614\n",
      "Model saved in path: /tmp/xor/model.ckpt-200\n",
      "Batch: 210, loss: 105.111, training accuracy: 78.00%\n",
      "W change:  0.0065034735\n",
      "Model saved in path: /tmp/xor/model.ckpt-210\n",
      "Batch: 220, loss: 98.302, training accuracy: 86.50%\n",
      "W change:  0.0071284445\n",
      "Model saved in path: /tmp/xor/model.ckpt-220\n",
      "Batch: 230, loss: 97.942, training accuracy: 86.00%\n",
      "W change:  0.0074893027\n",
      "Model saved in path: /tmp/xor/model.ckpt-230\n",
      "Batch: 240, loss: 88.834, training accuracy: 94.00%\n",
      "W change:  0.007769899\n",
      "Model saved in path: /tmp/xor/model.ckpt-240\n",
      "Batch: 250, loss: 86.090, training accuracy: 88.00%\n",
      "W change:  0.0073882076\n",
      "Model saved in path: /tmp/xor/model.ckpt-250\n",
      "Batch: 260, loss: 78.658, training accuracy: 94.50%\n",
      "W change:  0.0064323624\n",
      "Model saved in path: /tmp/xor/model.ckpt-260\n",
      "Batch: 270, loss: 78.097, training accuracy: 94.50%\n",
      "W change:  0.0062269075\n",
      "Model saved in path: /tmp/xor/model.ckpt-270\n",
      "Batch: 280, loss: 75.227, training accuracy: 94.50%\n",
      "W change:  0.005258039\n",
      "Model saved in path: /tmp/xor/model.ckpt-280\n",
      "Batch: 290, loss: 70.515, training accuracy: 94.00%\n",
      "W change:  0.0046341727\n",
      "Model saved in path: /tmp/xor/model.ckpt-290\n",
      "Batch: 300, loss: 60.760, training accuracy: 96.50%\n",
      "W change:  0.004426601\n",
      "Model saved in path: /tmp/xor/model.ckpt-300\n",
      "Batch: 310, loss: 57.483, training accuracy: 97.00%\n",
      "W change:  0.0039548306\n",
      "Model saved in path: /tmp/xor/model.ckpt-310\n",
      "Batch: 320, loss: 56.743, training accuracy: 95.50%\n",
      "W change:  0.0033593213\n",
      "Model saved in path: /tmp/xor/model.ckpt-320\n",
      "Batch: 330, loss: 51.972, training accuracy: 97.50%\n",
      "W change:  0.0030389105\n",
      "Model saved in path: /tmp/xor/model.ckpt-330\n",
      "Batch: 340, loss: 50.315, training accuracy: 96.50%\n",
      "W change:  0.0029056612\n",
      "Model saved in path: /tmp/xor/model.ckpt-340\n",
      "Batch: 350, loss: 47.201, training accuracy: 98.00%\n",
      "W change:  0.002562459\n",
      "Model saved in path: /tmp/xor/model.ckpt-350\n",
      "Batch: 360, loss: 47.652, training accuracy: 96.00%\n",
      "W change:  0.0021628607\n",
      "Model saved in path: /tmp/xor/model.ckpt-360\n",
      "Batch: 370, loss: 46.282, training accuracy: 98.00%\n",
      "W change:  0.002066087\n",
      "Model saved in path: /tmp/xor/model.ckpt-370\n",
      "Batch: 380, loss: 41.229, training accuracy: 98.50%\n",
      "W change:  0.0018053773\n",
      "Model saved in path: /tmp/xor/model.ckpt-380\n",
      "Batch: 390, loss: 38.993, training accuracy: 99.50%\n",
      "W change:  0.0015997645\n",
      "Model saved in path: /tmp/xor/model.ckpt-390\n",
      "Batch: 400, loss: 43.429, training accuracy: 97.50%\n",
      "W change:  0.0013877472\n",
      "Model saved in path: /tmp/xor/model.ckpt-400\n",
      "Batch: 410, loss: 37.434, training accuracy: 97.50%\n",
      "W change:  0.0013428065\n",
      "Model saved in path: /tmp/xor/model.ckpt-410\n",
      "Batch: 420, loss: 38.522, training accuracy: 96.00%\n",
      "W change:  0.0011565586\n",
      "Model saved in path: /tmp/xor/model.ckpt-420\n",
      "Batch: 430, loss: 37.027, training accuracy: 98.00%\n",
      "W change:  0.0010588284\n",
      "Model saved in path: /tmp/xor/model.ckpt-430\n",
      "Batch: 440, loss: 33.445, training accuracy: 98.50%\n",
      "W change:  0.0008713829\n",
      "Model saved in path: /tmp/xor/model.ckpt-440\n",
      "Batch: 450, loss: 31.166, training accuracy: 98.50%\n",
      "W change:  0.00087648846\n",
      "Model saved in path: /tmp/xor/model.ckpt-450\n",
      "Batch: 460, loss: 34.257, training accuracy: 97.00%\n",
      "W change:  0.00081488275\n",
      "Model saved in path: /tmp/xor/model.ckpt-460\n",
      "Batch: 470, loss: 27.849, training accuracy: 99.00%\n",
      "W change:  0.00077311986\n",
      "Model saved in path: /tmp/xor/model.ckpt-470\n",
      "Batch: 480, loss: 30.866, training accuracy: 98.00%\n",
      "W change:  0.00075295195\n",
      "Model saved in path: /tmp/xor/model.ckpt-480\n",
      "Batch: 490, loss: 25.813, training accuracy: 99.00%\n",
      "W change:  0.000609153\n",
      "Model saved in path: /tmp/xor/model.ckpt-490\n",
      "Batch: 500, loss: 26.878, training accuracy: 97.50%\n",
      "W change:  0.00051961525\n",
      "Model saved in path: /tmp/xor/model.ckpt-500\n",
      "Batch: 510, loss: 22.893, training accuracy: 99.00%\n",
      "W change:  0.00060327986\n",
      "Model saved in path: /tmp/xor/model.ckpt-510\n",
      "Batch: 520, loss: 23.697, training accuracy: 98.00%\n",
      "W change:  0.00051514385\n",
      "Model saved in path: /tmp/xor/model.ckpt-520\n",
      "Batch: 530, loss: 30.509, training accuracy: 99.00%\n",
      "W change:  0.00047469663\n",
      "Model saved in path: /tmp/xor/model.ckpt-530\n",
      "Batch: 540, loss: 21.979, training accuracy: 99.00%\n",
      "W change:  0.0004276753\n",
      "Model saved in path: /tmp/xor/model.ckpt-540\n",
      "Batch: 550, loss: 23.716, training accuracy: 99.00%\n",
      "W change:  0.00046372917\n",
      "Model saved in path: /tmp/xor/model.ckpt-550\n",
      "Batch: 560, loss: 18.074, training accuracy: 99.50%\n",
      "W change:  0.00040204645\n",
      "Model saved in path: /tmp/xor/model.ckpt-560\n",
      "Batch: 570, loss: 23.010, training accuracy: 100.00%\n",
      "W change:  0.00038339756\n",
      "Model saved in path: /tmp/xor/model.ckpt-570\n",
      "Batch: 580, loss: 19.939, training accuracy: 99.50%\n",
      "W change:  0.00036777926\n",
      "Model saved in path: /tmp/xor/model.ckpt-580\n",
      "Batch: 590, loss: 24.895, training accuracy: 98.50%\n",
      "W change:  0.00037417084\n",
      "Model saved in path: /tmp/xor/model.ckpt-590\n",
      "Batch: 600, loss: 21.968, training accuracy: 99.00%\n",
      "W change:  0.00038368735\n",
      "Model saved in path: /tmp/xor/model.ckpt-600\n",
      "Batch: 610, loss: 19.032, training accuracy: 98.50%\n",
      "W change:  0.0003725757\n",
      "Model saved in path: /tmp/xor/model.ckpt-610\n",
      "Batch: 620, loss: 22.107, training accuracy: 99.50%\n",
      "W change:  0.00033848936\n",
      "Model saved in path: /tmp/xor/model.ckpt-620\n",
      "Batch: 630, loss: 23.555, training accuracy: 99.50%\n",
      "W change:  0.0003084645\n",
      "Model saved in path: /tmp/xor/model.ckpt-630\n",
      "Batch: 640, loss: 19.158, training accuracy: 100.00%\n",
      "W change:  0.0002534451\n",
      "Model saved in path: /tmp/xor/model.ckpt-640\n",
      "Batch: 650, loss: 21.994, training accuracy: 99.00%\n",
      "W change:  0.0003676961\n",
      "Model saved in path: /tmp/xor/model.ckpt-650\n",
      "Batch: 660, loss: 20.924, training accuracy: 100.00%\n",
      "W change:  0.0004686324\n",
      "Model saved in path: /tmp/xor/model.ckpt-660\n",
      "Batch: 670, loss: 20.332, training accuracy: 99.00%\n",
      "W change:  0.0002616564\n",
      "Model saved in path: /tmp/xor/model.ckpt-670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 680, loss: 21.063, training accuracy: 99.50%\n",
      "W change:  0.0002719172\n",
      "Model saved in path: /tmp/xor/model.ckpt-680\n",
      "Batch: 690, loss: 16.920, training accuracy: 99.50%\n",
      "W change:  0.00024256074\n",
      "Model saved in path: /tmp/xor/model.ckpt-690\n",
      "Batch: 700, loss: 18.221, training accuracy: 100.00%\n",
      "W change:  0.0002848062\n",
      "Model saved in path: /tmp/xor/model.ckpt-700\n",
      "Batch: 710, loss: 17.502, training accuracy: 99.50%\n",
      "W change:  0.00021081\n",
      "Model saved in path: /tmp/xor/model.ckpt-710\n",
      "Batch: 720, loss: 16.036, training accuracy: 97.50%\n",
      "W change:  0.00026090638\n",
      "Model saved in path: /tmp/xor/model.ckpt-720\n",
      "Batch: 730, loss: 12.693, training accuracy: 100.00%\n",
      "W change:  0.00021241183\n",
      "Model saved in path: /tmp/xor/model.ckpt-730\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('sparse_fully_connected', reuse=True):\n",
    "    W = tf.get_variable('W', shape=[50])\n",
    "    \n",
    "\n",
    "# run the training\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    W0 = W.eval()\n",
    "    sess.run(train_init_op)\n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            l, _, acc = sess.run([loss, optimizer, accuracy])\n",
    "            if i % 10 == 0:\n",
    "                W1 = W.eval()\n",
    "                print(\"Batch: {}, loss: {:.3f}, training accuracy: {:.2f}%\".format(i, l, acc * 100))\n",
    "                print(\"W change: \", np.sum((W0-W1)**2))\n",
    "                W0 = W1\n",
    "                save_path = saver.save(sess, model_dir, i)\n",
    "                print(\"Model saved in path:\", save_path)\n",
    "            i += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    # now setup the validation run\n",
    "    valid_iters = 100\n",
    "    # re-initialize the iterator, but this time with validation data\n",
    "    sess.run(test_init_op)\n",
    "    avg_acc = 0\n",
    "    valid_iters = 0\n",
    "    while True:\n",
    "        try:\n",
    "            acc = sess.run([accuracy])\n",
    "            avg_acc += acc[0]\n",
    "            valid_iters += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    print(\"Average validation set accuracy over {} iterations is {:.2f}%\".format(valid_iters, (avg_acc / valid_iters) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sparse_fully_connected', reuse=True):\n",
    "    W = tf.get_variable('W', shape=[50])\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, model_dir)\n",
    "    print(W.eval())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
